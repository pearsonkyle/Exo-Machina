# Exo-Machina
A deep language model, GPT-2, is trained on scientific manuscripts from [ArXiv](https://arxiv.org/). This pilot study uses abstracts from ~1.7M articles as training data in order to explore correlations in scientific literature from a language modelling perspective. A language models are algorithms used to generate sequences of numers that correspond to tokens or words and can be used to represent sentances. The text samples are fed into the [GPT-2](https://openai.com/blog/better-language-models/) 117M and 774M model and trained for ~500,000 steps with fine tuning. After training, the language model is used to generate embeddings for each manuscript which can be clustered for visualization applications and queried for entity searches.

- ### [View on Hugging Face API](https://huggingface.co/pearsonkyle/gpt2-exomachina?text=We+can+remotely+sense+an+atmosphere+by+observing+its+reflected%2C+transmitted%2C+or+emitted+light+in+varying+geometries.+This+light+will+contain+information+on+the+planetary+conditions+including)

### Get started fast:

```python
from transformers import pipeline

exo = pipeline('text-generation',model='pearsonkyle/gpt2-exomachina', tokenizer='gpt2', config={'max_length':1600})
machina = lambda text: exo(text)[0]['generated_text']
```

For the large model (GPT-2 774M) use: `pearsonkyle/gpt2-exomachina-large` (Coming soon...)


![](Figures/exoplanet_keywords.png)

A few generated samples are below: 

- *We can remotely sense an atmosphere by observing its reflected, transmitted, or emitted light in varying geometries. This light will contain information on the planetary conditions including* `temperature, pressure, composition, and cloud optical thickness. One such property that is important is...`
- *The reflectance of Earth's vegetation suggests*
`that large, deciduous forest fires are composed of mostly dry, unprocessed material that is distributed in a nearly patchy fashion. The distributions of these fires are correlated with temperature, and also with vegetation...`
- *Directly imaged exoplanets probe* `key aspects of planet formation and evolution theory, as well as atmospheric and interior physics. These insights have led to numerous direct imaging instruments for exoplanets, many using polarimetry. However, current instruments take`


## Dependencies 

Set up your local python environment using: `conda env create -f environment.yml`

Or, use pip: `pip install -r requirements.txt`

## Training Samples

~2.1 million abstracts from the [Arxiv](https://arxiv.org/) available on [Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv). Also, see this Hugging Face [dataset](https://huggingface.co/datasets/scientific_papers) for more text data including the entire PDF parsed into text.

## Pre-processing for text generation/embedding
After downloading the Arxiv json above, run the following to conver it into a sqlite database:
    1. `python database.py` to create sql database
    2. `python json2db.py` to populate the db with json data from arxiv

## Train on a custom datset
Train a language model using the commands below:
    1. `python db2txt.py` to create a text file with one abstract per line
    2. `python train.py` to train a GPT-2 model

Interested in training this model in the cloud? Try this repo on [Google Colab](https://colab.research.google.com/drive/1Pur0rFi5YVdn7axYRacXWFMic4NxRexV?usp=sharing) )

## Add to the database with NASA ADS

Use NASA's Astrophysical Data System (ADS) to add more abstracts to the database. See `query_ads.py -h` for more details.

`python query_ads.py -q "transiting exoplanets"`

## Embeddings

The language model is used to generate embeddings for each manuscript which can be clustered for visualization applications and queried for entity searches. The embeddings are generated by taking the last hidden state of the model and averaging over the tokens in the text. The embeddings are then clustered using an approximate nearest neighbor technique (ANNOY) and queried with FAISS. The embeddings are also used to generate a [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) matrix for entity search.

## Nearest Neighbor Recommendations

Build a nearest neighbor tree from the text embeddings and use it to recommend similar abstracts

`python text_to_vec.py`

## Example Demo

Text generation and nearest neighbor recommendations in a single app:

`python -m bokeh serve --show bokeh_example.py`

## Convert to Hugging Face API

TODO

## Upload to iOS

`python gpt2_to_coreml.py`



References
- https://huggingface.co/roberta-base 
- https://huggingface.co/docs
- https://huggingface.co/transformers/training.html
- https://huggingface.co/transformers/notebooks.html
- https://colab.research.google.com/drive/1vsCh85T_Od7RBwXfvh1iysV-vTxmWXQO#scrollTo=ljknzOlNoyrv
- http://jalammar.github.io/illustrated-gpt2/
- https://github.com/huggingface/swift-coreml-transformers.git
